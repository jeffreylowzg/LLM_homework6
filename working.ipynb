{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreylowzg/LLM_homework6/blob/jeffrey-commits/data_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvPJTNsAqunz",
        "outputId": "9e9cae4c-f992-42c4-e774-2401221c5ea6"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"huggingface_hub[cli]\"\n",
        "!pip install torch transformers[torch] numpy tqdm datasets peft accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDQMBJbqmUU"
      },
      "source": [
        "Download dataset and saves 5%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqmc6jJNqmUW",
        "outputId": "23d91b9d-baee-400c-bd69-26e8ed7a97a4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"dmitva/human_ai_generated_text\", split=\"train\")\n",
        "\n",
        "# Calculate 5% of the dataset size\n",
        "sample_size = int(0.05 * len(dataset))\n",
        "\n",
        "# Sample 5% of the data\n",
        "sampled_dataset = dataset.shuffle(seed=42).select(range(sample_size))\n",
        "\n",
        "# Convert to pandas DataFrame for easier handling\n",
        "df = pd.DataFrame(sampled_dataset)\n",
        "\n",
        "# Ensure the 'data' directory exists\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Save to a CSV file in the 'data' folder\n",
        "df.to_csv(\"data/sample_5_percent.csv\", index=False)\n",
        "\n",
        "print(\"5% of the dataset has been saved to 'data/sample_5_percent.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZbkxsPgqmUY"
      },
      "source": [
        "Read saved data and split into labels 0 (for human) and 1 (for ai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epmrNu8IqmUZ",
        "outputId": "b13e4de9-0e51-4403-c482-eabcd528dc77"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Read the sampled CSV file\n",
        "df = pd.read_csv(\"data/sample_5_percent.csv\")\n",
        "\n",
        "# Initialize an empty list to hold the new records\n",
        "data = []\n",
        "\n",
        "# Process each row to create two entries: one for human text, one for AI text\n",
        "for _, row in df.iterrows():\n",
        "    # Append the human text with label 0\n",
        "    data.append({\n",
        "        \"text\": row[\"human_text\"],\n",
        "        \"instructions\": row[\"instructions\"],\n",
        "        \"label\": 0\n",
        "    })\n",
        "\n",
        "    # Append the AI text with label 1\n",
        "    data.append({\n",
        "        \"text\": row[\"ai_text\"],\n",
        "        \"instructions\": row[\"instructions\"],\n",
        "        \"label\": 1\n",
        "    })\n",
        "\n",
        "# Save the processed data to a JSON file\n",
        "outfile = \"data/sample_5_percent.jsonl\"\n",
        "with open(outfile, \"w\") as f:\n",
        "    for d in data:\n",
        "        json.dump(d, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(f\"The dataset has been saved to {outfile} with the specified format.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "split dataset into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths\n",
        "original_data_path = \"data/sample_5_percent.jsonl\"\n",
        "train_data_path = \"data/train.jsonl\"\n",
        "test_data_path = \"data/test.jsonl\"\n",
        "\n",
        "# Function to split JSONL file\n",
        "def split_jsonl_file(input_path, train_path, test_path, test_size=0.2):\n",
        "    with open(input_path, \"r\") as f:\n",
        "        lines = [json.loads(line) for line in f]\n",
        "    \n",
        "    train_lines, test_lines = train_test_split(lines, test_size=test_size, random_state=42)\n",
        "    \n",
        "    # Save split datasets\n",
        "    with open(train_path, \"w\") as train_file:\n",
        "        for line in train_lines:\n",
        "            train_file.write(json.dumps(line) + \"\\n\")\n",
        "    \n",
        "    with open(test_path, \"w\") as test_file:\n",
        "        for line in test_lines:\n",
        "            test_file.write(json.dumps(line) + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Perform the split\n",
        "    split_jsonl_file(original_data_path, train_data_path, test_data_path)\n",
        "    print(f\"Data split completed. Train: {train_data_path}, Test: {test_data_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FzZwr7yXqmUZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching 8 files:   0%|                                   | 0/8 [00:00<?, ?it/s]Downloading 'README.md' to 'models/pythia-160m/.cache/huggingface/download/README.md.2e8b2b93bf534833600f862c90c0b53fc6f76a46.incomplete'\n",
            "Downloading 'special_tokens_map.json' to 'models/pythia-160m/.cache/huggingface/download/special_tokens_map.json.0204ed10c186a4c7c68f55dff8f26087a45898d6.incomplete'\n",
            "Downloading 'pytorch_model.bin' to 'models/pythia-160m/.cache/huggingface/download/pytorch_model.bin.8d856725c4a8266f10568cc1269948fc851e2aed1fb230036af2eac68e9df072.incomplete'\n",
            "\n",
            "README.md: 100%|███████████████████████████| 13.6k/13.6k [00:00<00:00, 26.4MB/s]\u001b[A\n",
            "Download complete. Moving file to models/pythia-160m/README.md\n",
            "Downloading 'config.json' to 'models/pythia-160m/.cache/huggingface/download/config.json.b8368ff94f3bcf3088de5e9912251fc0208ae524.incomplete'\n",
            "\n",
            "special_tokens_map.json: 100%|███████████████| 99.0/99.0 [00:00<00:00, 1.16MB/s]\u001b[A\n",
            "Download complete. Moving file to models/pythia-160m/special_tokens_map.json\n",
            "Downloading 'tokenizer_config.json' to 'models/pythia-160m/.cache/huggingface/download/tokenizer_config.json.f1860edb10f80bcaf7b023fce47c68a23b724c23.incomplete'\n",
            "\n",
            "config.json: 100%|██████████████████████████████| 569/569 [00:00<00:00, 947kB/s]\u001b[A\n",
            "Download complete. Moving file to models/pythia-160m/config.json\n",
            "Downloading 'model.safetensors' to 'models/pythia-160m/.cache/huggingface/download/model.safetensors.29d2e457a664e41c12c735f20a36dc0956a665f614a54ce5db21a32e75965270.incomplete'\n",
            "\n",
            "tokenizer_config.json: 100%|███████████████████| 396/396 [00:00<00:00, 5.79MB/s]\u001b[A\n",
            "Download complete. Moving file to models/pythia-160m/tokenizer_config.json\n",
            "Downloading 'tokenizer.json' to 'models/pythia-160m/.cache/huggingface/download/tokenizer.json.f74dfbfab8f97770a87769c739fb080c21c8bacc.incomplete'\n",
            "\n",
            "model.safetensors:   0%|                             | 0.00/375M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer.json:   0%|                               | 0.00/2.11M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading '.gitattributes' to 'models/pythia-160m/.cache/huggingface/download/.gitattributes.c7d9f3332a950355d5a77d85000f05e6f45435ea.incomplete'\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 100%|██████████████████████| 1.48k/1.48k [00:00<00:00, 2.20MB/s]\u001b[A\u001b[A\u001b[A\n",
            "Download complete. Moving file to models/pythia-160m/.gitattributes\n",
            "Fetching 8 files:  12%|███▍                       | 1/8 [00:00<00:03,  2.32it/s]\n",
            "\n",
            "tokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:00<00:00, 9.86MB/s]\u001b[A\u001b[A\n",
            "Download complete. Moving file to models/pythia-160m/tokenizer.json\n",
            "\n",
            "\n",
            "pytorch_model.bin:   0%|                             | 0.00/375M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model.safetensors:   3%|▌                   | 10.5M/375M [00:00<00:10, 34.4MB/s]\u001b[A\n",
            "model.safetensors:   6%|█                   | 21.0M/375M [00:00<00:13, 25.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:   3%|▌                   | 10.5M/375M [00:00<00:18, 19.7MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:   8%|█▋                  | 31.5M/375M [00:01<00:14, 24.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:   6%|█                   | 21.0M/375M [00:01<00:21, 16.8MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  11%|██▏                 | 41.9M/375M [00:01<00:13, 24.3MB/s]\u001b[A\n",
            "model.safetensors:  14%|██▊                 | 52.4M/375M [00:02<00:13, 24.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:   8%|█▋                  | 31.5M/375M [00:01<00:22, 15.5MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  17%|███▎                | 62.9M/375M [00:02<00:13, 23.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  11%|██▏                 | 41.9M/375M [00:02<00:21, 15.8MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  20%|███▉                | 73.4M/375M [00:03<00:13, 23.0MB/s]\u001b[A\n",
            "model.safetensors:  22%|████▍               | 83.9M/375M [00:03<00:12, 22.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  14%|██▊                 | 52.4M/375M [00:03<00:20, 15.8MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  25%|█████               | 94.4M/375M [00:04<00:12, 22.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  17%|███▎                | 62.9M/375M [00:03<00:19, 16.4MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  28%|█████▊               | 105M/375M [00:04<00:12, 21.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  20%|███▉                | 73.4M/375M [00:04<00:17, 17.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  22%|████▍               | 83.9M/375M [00:04<00:15, 19.0MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  31%|██████▍              | 115M/375M [00:05<00:13, 19.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  25%|█████               | 94.4M/375M [00:05<00:14, 19.0MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  34%|███████              | 126M/375M [00:05<00:12, 19.4MB/s]\u001b[A\n",
            "model.safetensors:  36%|███████▋             | 136M/375M [00:06<00:11, 20.7MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  28%|█████▊               | 105M/375M [00:06<00:15, 17.4MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  39%|████████▏            | 147M/375M [00:06<00:10, 21.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  31%|██████▍              | 115M/375M [00:06<00:15, 17.1MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  42%|████████▊            | 157M/375M [00:07<00:10, 21.4MB/s]\u001b[A\n",
            "model.safetensors:  45%|█████████▍           | 168M/375M [00:07<00:09, 22.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  34%|███████              | 126M/375M [00:07<00:15, 15.6MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  48%|█████████▉           | 178M/375M [00:07<00:08, 23.4MB/s]\u001b[A\n",
            "model.safetensors:  50%|██████████▌          | 189M/375M [00:08<00:07, 24.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  36%|███████▋             | 136M/375M [00:08<00:16, 14.4MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  53%|███████████▏         | 199M/375M [00:08<00:07, 24.7MB/s]\u001b[A\n",
            "model.safetensors:  56%|███████████▋         | 210M/375M [00:09<00:06, 25.5MB/s]\u001b[A\n",
            "model.safetensors:  59%|████████████▎        | 220M/375M [00:09<00:06, 25.6MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  39%|████████▏            | 147M/375M [00:09<00:17, 13.3MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  62%|████████████▉        | 231M/375M [00:09<00:05, 25.4MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  42%|████████▊            | 157M/375M [00:09<00:15, 13.9MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  64%|█████████████▌       | 241M/375M [00:10<00:05, 24.3MB/s]\u001b[A\n",
            "model.safetensors:  67%|██████████████       | 252M/375M [00:10<00:05, 23.9MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  45%|█████████▍           | 168M/375M [00:10<00:14, 14.2MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  70%|██████████████▋      | 262M/375M [00:11<00:05, 22.3MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  48%|█████████▉           | 178M/375M [00:11<00:12, 15.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  50%|██████████▌          | 189M/375M [00:11<00:10, 17.1MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  73%|███████████████▎     | 273M/375M [00:12<00:05, 20.0MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  53%|███████████▏         | 199M/375M [00:12<00:09, 18.0MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  75%|███████████████▊     | 283M/375M [00:12<00:04, 19.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  56%|███████████▋         | 210M/375M [00:12<00:09, 17.9MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  78%|████████████████▍    | 294M/375M [00:13<00:04, 20.0MB/s]\u001b[A\n",
            "model.safetensors:  81%|█████████████████    | 304M/375M [00:13<00:03, 21.5MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  59%|████████████▎        | 220M/375M [00:13<00:09, 16.2MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  84%|█████████████████▌   | 315M/375M [00:13<00:02, 22.3MB/s]\u001b[A\n",
            "model.safetensors:  87%|██████████████████▏  | 325M/375M [00:14<00:02, 23.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  62%|████████████▉        | 231M/375M [00:14<00:09, 15.3MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  89%|██████████████████▊  | 336M/375M [00:14<00:01, 23.4MB/s]\u001b[A\n",
            "model.safetensors:  92%|███████████████████▍ | 346M/375M [00:15<00:01, 23.2MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  64%|█████████████▌       | 241M/375M [00:15<00:08, 15.3MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  95%|███████████████████▉ | 357M/375M [00:15<00:00, 22.1MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  67%|██████████████       | 252M/375M [00:15<00:07, 16.2MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  98%|████████████████████▌| 367M/375M [00:16<00:00, 20.8MB/s]\u001b[A\n",
            "\n",
            "pytorch_model.bin:  70%|██████████████▋      | 262M/375M [00:16<00:06, 17.1MB/s]\u001b[A\u001b[A\n",
            "model.safetensors: 100%|█████████████████████| 375M/375M [00:16<00:00, 22.2MB/s]\u001b[A\n",
            "Download complete. Moving file to models/pythia-160m/model.safetensors\n",
            "Fetching 8 files:  50%|█████████████▌             | 4/8 [00:17<00:18,  4.61s/it]\n",
            "\n",
            "pytorch_model.bin:  73%|███████████████▎     | 273M/375M [00:16<00:05, 18.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  75%|███████████████▊     | 283M/375M [00:16<00:04, 21.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  78%|████████████████▍    | 294M/375M [00:17<00:03, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  81%|█████████████████    | 304M/375M [00:17<00:02, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  84%|█████████████████▌   | 315M/375M [00:17<00:02, 26.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  87%|██████████████████▏  | 325M/375M [00:18<00:01, 29.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  89%|██████████████████▊  | 336M/375M [00:18<00:01, 31.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  92%|███████████████████▍ | 346M/375M [00:18<00:00, 33.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  95%|███████████████████▉ | 357M/375M [00:19<00:00, 26.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin:  98%|████████████████████▌| 367M/375M [00:19<00:00, 28.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "pytorch_model.bin: 100%|█████████████████████| 375M/375M [00:19<00:00, 18.8MB/s]\u001b[A\u001b[A\n",
            "Download complete. Moving file to models/pythia-160m/pytorch_model.bin\n",
            "Fetching 8 files: 100%|███████████████████████████| 8/8 [00:20<00:00,  2.56s/it]\n",
            "/mnt/c/Documents/Admin/CMU_Masters_admin/Module_Stuff/11667_LLMs/homework6/models/pythia-160m\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p models/pythia-160m\n",
        "!huggingface-cli download EleutherAI/pythia-160m --local-dir ./models/pythia-160m\n",
        "!cd ../.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnp2eBMcwxDb",
        "outputId": "4fc57d9b-025b-4b59-cc54-498baa539f6b"
      },
      "outputs": [],
      "source": [
        "!wandb login 7077b7416aa6d8dd6e87ab0b9150b82abed30bd1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train + evaluate (freeze 6 layers + lora r(16) alpha(32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Paths for train and test data\n",
        "train_data_path = \"data/train.jsonl\"\n",
        "test_data_path = \"data/test.jsonl\"\n",
        "\n",
        "# Specify the local directory where the model was downloaded\n",
        "model_path = \"./models/pythia-160m\"\n",
        "\n",
        "# Load the tokenizer and model for sequence classification\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)  # Binary classification\n",
        "\n",
        "# Add padding token if it doesn't exist and set it as the pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings to match the new pad token\n",
        "\n",
        "# Explicitly set pad_token_id in model configuration\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",   # Sequence classification\n",
        "    inference_mode=False,\n",
        "    r=16,                  # LoRA rank\n",
        "    lora_alpha=32,         # Scaling factor\n",
        "    lora_dropout=0.1       # Regularization\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Freeze the first few layers of GPT-NeoX\n",
        "num_layers_to_freeze = 6  # Adjust based on model depth and dataset size\n",
        "\n",
        "# For GPT-NeoX, transformer layers are in model.base_model.gpt_neox.layers\n",
        "for layer in model.base_model.gpt_neox.layers[:num_layers_to_freeze]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Always ensure the classification head and LoRA layers are trainable\n",
        "model.print_trainable_parameters()  # Check trainable parameters\n",
        "\n",
        "# Load the split datasets\n",
        "train_dataset = load_dataset(\"json\", data_files=train_data_path)[\"train\"]\n",
        "test_dataset = load_dataset(\"json\", data_files=test_data_path)[\"train\"]\n",
        "\n",
        "# Preprocessing function for tokenization and label mapping\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    inputs[\"labels\"] = examples[\"label\"]  # Use label for classification\n",
        "    return inputs\n",
        "\n",
        "# Tokenize the datasets\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Define a function to compute accuracy\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)  # Take the highest probability class\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./models/pythia-160m-finetuned-classifier-lora\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    save_strategy=\"epoch\",     # Save the model at the end of each epoch\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    learning_rate=1e-4,        # Adjusted for PEFT\n",
        "    fp16=True,                 # Enable mixed precision training if supported\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "model.save_pretrained(\"./models/pythia-160m-finetuned-classifier-lora\")\n",
        "tokenizer.save_pretrained(\"./models/pythia-160m-finetuned-classifier-lora\")\n",
        "\n",
        "print(\"Model fine-tuning completed and saved to './models/pythia-160m-finetuned-classifier-lora'\")\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation Results: {eval_results}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation on untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Specify the paths for the train and test datasets\n",
        "train_data_path = \"data/train.jsonl\"\n",
        "test_data_path = \"data/test.jsonl\"\n",
        "\n",
        "# Specify the local directory where the model was downloaded\n",
        "model_path = \"./models/pythia-160m\"\n",
        "\n",
        "# Load the tokenizer and model for sequence classification\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)  # Binary classification\n",
        "\n",
        "# Add padding token if it doesn't exist and set it as the pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings to match the new pad token\n",
        "\n",
        "# Explicitly set pad_token_id in model configuration\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",   # Sequence classification\n",
        "    inference_mode=False,\n",
        "    r=16,                  # LoRA rank\n",
        "    lora_alpha=32,         # Scaling factor\n",
        "    lora_dropout=0.1       # Regularization\n",
        ")\n",
        "\n",
        "# Wrap the model with LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Load the train and test datasets\n",
        "train_dataset = load_dataset(\"json\", data_files=train_data_path)[\"train\"]\n",
        "test_dataset = load_dataset(\"json\", data_files=test_data_path)[\"train\"]\n",
        "\n",
        "# Preprocessing function for tokenization and label mapping\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    inputs[\"labels\"] = examples[\"label\"]  # Use label for classification\n",
        "    return inputs\n",
        "\n",
        "# Tokenize the datasets\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Define a function to compute accuracy\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)  # Take the highest probability class\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Set up evaluation arguments\n",
        "evaluation_args = TrainingArguments(\n",
        "    output_dir=\"./models/pythia-160m-eval\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    fp16=True,  # Enable mixed precision evaluation if supported\n",
        ")\n",
        "\n",
        "# Initialize the Trainer for evaluation only\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=evaluation_args,\n",
        "    train_dataset=tokenized_train_dataset,  # Optional: If you're training as well\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Evaluate the untrained model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation Results (Untrained Model): {eval_results}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "printing generated outputs before classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPTNeoXForSequenceClassification were not initialized from the model checkpoint at ./models/pythia-160m and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 softmax values:\n",
            "1: Index 650, Value 0.13100165128707886\n",
            "2: Index 544, Value 0.0750051960349083\n",
            "3: Index 212, Value 0.06678420305252075\n",
            "4: Index 759, Value 0.052072495222091675\n",
            "5: Index 69, Value 0.03131868690252304\n",
            "6: Index 632, Value 0.029305055737495422\n",
            "7: Index 78, Value 0.025107571855187416\n",
            "8: Index 423, Value 0.023854373022913933\n",
            "9: Index 372, Value 0.023483315482735634\n",
            "10: Index 286, Value 0.020786363631486893\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import json\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel\n",
        "\n",
        "# Path to the fine-tuned model and test data\n",
        "model_path = \"./models/pythia-160m-finetuned-classifier-lora\"\n",
        "base_model_path = \"./models/pythia-160m\"  # Base pre-trained model path\n",
        "test_data_path = \"data/test.jsonl\"\n",
        "\n",
        "# Load the tokenizer from the fine-tuned directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Add padding token if not already defined\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Load the base model\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    base_model_path,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Resize the base model's embedding layer to match the tokenizer\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set the padding token ID in the model configuration\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Load the LoRA adapters into the resized base model\n",
        "model = PeftModel.from_pretrained(base_model, model_path)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = load_dataset(\"json\", data_files=test_data_path)[\"train\"]\n",
        "\n",
        "# Extract the text prompts from the dataset\n",
        "test_prompts = test_dataset[\"text\"][:10]  # Select only the first 10 inputs\n",
        "\n",
        "# Tokenize the test prompts\n",
        "inputs = tokenizer(\n",
        "    test_prompts,\n",
        "    padding=True,  # Enable padding\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Move tensors to the appropriate device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Pass the inputs through the model to get hidden states\n",
        "with torch.no_grad():\n",
        "    outputs = model.base_model(**inputs, output_hidden_states=True)\n",
        "    # Extract the last hidden state (before the classification head)\n",
        "    hidden_states = outputs.hidden_states[-1]  # Last layer's hidden states\n",
        "    pooled_embeddings = hidden_states[:, 0, :]  # CLS token's embedding for each prompt\n",
        "\n",
        "# Convert embeddings to a numpy array for saving\n",
        "pooled_embeddings_np = pooled_embeddings.cpu().numpy()\n",
        "\n",
        "# Compute softmax values for pooled_embeddings_np[0]\n",
        "softmax_values = torch.nn.functional.softmax(torch.tensor(pooled_embeddings_np[1]), dim=0).numpy()\n",
        "\n",
        "# Get the top 10 highest values and their indices\n",
        "top_10_indices = np.argsort(softmax_values)[-10:][::-1] \n",
        "top_10_values = softmax_values[top_10_indices]\n",
        "\n",
        "# Print the top 10 highest values and their corresponding indices\n",
        "print(\"Top 10 softmax values:\")\n",
        "for i, (index, value) in enumerate(zip(top_10_indices, top_10_values)):\n",
        "    print(f\"{i + 1}: Index {index}, Value {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Temp workspace for generative output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "import json\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"dmitva/human_ai_generated_text\", split=\"train\")\n",
        "train_testvalid = dataset.train_test_split(test_size=0.2)\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.2)\n",
        "dataset = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'dev': test_valid['train']})\n",
        "\n",
        "for split in [\"train\", \"test\", \"dev\"]: \n",
        "    with open(f\"data/processed_{split}.jsonl\", \"w\") as f: \n",
        "    # Process each row to create two entries: one for human text, one for AI text\n",
        "        for row in dataset[split].iter(batch_size=1):\n",
        "            # Append the human text with label 0\n",
        "            human = {\n",
        "                \"text\": row[\"human_text\"][0],\n",
        "                \"instructions\": row[\"instructions\"][0],\n",
        "                \"label\": 0\n",
        "            }\n",
        "\n",
        "            # Append the AI text with label 1\n",
        "            ai = {\n",
        "                \"text\": row[\"ai_text\"][0],\n",
        "                \"instructions\": row[\"instructions\"][0],\n",
        "                \"label\": 1\n",
        "            }\n",
        "\n",
        "            json.dump(human, f)\n",
        "            f.write(\"\\n\")\n",
        "            json.dump(ai, f)\n",
        "            f.write(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from functools import partial\n",
        "import datasets\n",
        "\n",
        "def _create_prompt(entry, instruct_key=\"instructions\", response_key=\"text\"): \n",
        "    label = \"human\" if entry[\"label\"] == 0 else \"AI\"\n",
        "    prompt = f\"Based on the task instruction, determine if the response is written by a human, or AI generated.\\nInstruction: {entry[instruct_key]}\\n\\nResponse: {entry[response_key]}\\n\\nThe response is written by: {label}\"\n",
        "\n",
        "    entry[\"input_text\"] = prompt\n",
        "    return entry\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "\n",
        "    return tokenizer(\n",
        "        batch[\"input_text\"],\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, dataset: datasets.Dataset):\n",
        "    \"\"\"Format & tokenize it so it is ready for training\n",
        "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
        "    \"\"\"\n",
        "    \n",
        "    dataset = dataset.map(_create_prompt)\n",
        "\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
        "    processed_dat = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names,\n",
        "    )\n",
        "\n",
        "    processed_dat = processed_dat.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "    \n",
        "    return processed_dat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "embedding vocabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[13961]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[18128]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.encode(\"AI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 0: human > AI\n",
            "Prompt 1: human > AI\n",
            "Prompt 2: human > AI\n",
            "Prompt 3: human > AI\n",
            "Prompt 4: AI > human\n",
            "Prompt 5: human > AI\n",
            "Prompt 6: human > AI\n",
            "Prompt 7: human > AI\n",
            "Prompt 8: human > AI\n",
            "Prompt 9: human > AI\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Paths to model and dataset\n",
        "base_model_path = \"./models/pythia-160m\"\n",
        "test_data_path = \"data/test.jsonl\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Load the base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        ")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = load_dataset(\"json\", data_files=test_data_path)[\"train\"]\n",
        "test_prompts = test_dataset[\"text\"][:10]  # Select the first 10 prompts\n",
        "\n",
        "# Tokenize the test prompts\n",
        "inputs = tokenizer(\n",
        "    test_prompts,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Move tensors to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "base_model.to(device)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Indices to extract\n",
        "token_indices = [tokenizer.encode(\"human\"), tokenizer.encode(\"AI\")]\n",
        "\n",
        "# Extract the last hidden states\n",
        "with torch.no_grad():\n",
        "    outputs = base_model(**inputs, output_hidden_states=True)\n",
        "\n",
        "# Loop through all prompts dynamically based on the dataset\n",
        "for i, prompt in enumerate(test_prompts, start=0):\n",
        "    # Calculate the logits for \"AI\" and \"human\"\n",
        "    AI_idx = outputs.logits[i, -1, tokenizer.encode(\"AI\")].detach().cpu().numpy()\n",
        "    human_idx = outputs.logits[i, -1, tokenizer.encode(\"human\")].detach().cpu().numpy()\n",
        "\n",
        "    # Compare logits and print results accordingly\n",
        "    if human_idx > AI_idx:\n",
        "        print(f\"Prompt {i}: human > AI\")\n",
        "    else:\n",
        "        print(f\"Prompt {i}: AI > human\")\n",
        "\n",
        "    \n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Few-Shot Prompt Used:\n",
            "Text: He became the owner of his own brokerage firm and went on to lead a successful life. He was a living testament that one can rise above the circumstances of their life and achieve success against all odds. He rose from a tumultuous childhood and poverty to become a successful entrepreneur, stockbroker, and author. Chris had a great impact on society and his story will continue to be a source of hope and inspiration.. He wrote numerous books and appeared in major films, magnifying his reach and inspiring others.\n",
            "\n",
            "Chris's journey of resilience serves a source of inspiration to many. He put in the work to earn his degree in business administration. He credited his strong will to stay in school and have a better life as a key factor in his success.\n",
            "\n",
            "Thanks to Chris's relentless determination and hard work, he was able to break barriers within the world of banking and finance. \n",
            "Chris Gardner's story is an inspiring one. Despite facing numerous personal and professional challenges, he managed to stay on top of his studies\n",
            "Label: Human-generated\n",
            "\n",
            "Text: That's what I know about the guide careers, the student want knowlegde from when they're in high chool.. If the school's have guide careers or activities of work the students, they will enjoy that.\n",
            "\n",
            "The seconds reasons is personal but, I see that almost the all people of my country when they come here and they have the opportunity to study but, when they end from the high school they stay in the same thing working in not good jobs earning 14 dollars per hours and some people say now that the diploma from high school is nothing to their. Some people go to school beacuse it's something ilegal, if they not going, I think they will put more interest to the study if they have the opportunity to study what they want to be when they are more olders. The school's need give the opportunity to prepare the students. I know we have young age but we want to be something our life, we have the goals to work in something, we love. I think that the schools need to change to give the student to choose, what they want know, I not talking about the classes, I talking about careers guides. Maybe the all student is not rich's but the only thing they want is work what they want.\n",
            "\n",
            "Everyone enjoy doing the thing what they love with the heart.\n",
            "\n",
            "I lucky to be in this school because the school give the opportunity to some students go to tech. This are examples from others people too.\n",
            "\n",
            "The students dreams that the school have something similar from college.\n",
            "\n",
            "Some people tell me that when I end from high school, I will work in the same jobs, maybe I think the same because my family don't have the money but if the school's have something of academic's to know about the jobs, maybe the academic is not free but, is more cheeper than college or the school's have something that we can know about the jobs. I'm agree to take guides about careers because, I think that the students include to me, we want to have a spacific career guide when, we enter to high school because when we will go to collage, we have to be prepared with ideas, how is the career that we want to take and the students has to be ready with knowlegde with his own careers.\n",
            "\n",
            "The first reason, I say that because the students will be more excite to came to the school, they will say, I like to go to shool beacuse, I practice somenthing that I love\n",
            "Label: AI-generated\n",
            "\n",
            "Text: Once I reach there, I will eat their famous dishes, explore the major cities, meet many people and watch animes.. Well, Japan is a place which is famous for most of the things I have mentioned.\n",
            "\n",
            "I want to visit Japan because it's a place famous for their culture, tradition, advanced cities, technology and food. Another reason why I want to visit is because, Japan ranks at Top 10 for having low crime rates in the world and Japan ranks Top 5 for being a busiest country in the world.\n",
            "\n",
            "I've seen many YouTubers making video of their stay in Japan and how they enjoyed staying there. Another reason for me to visit Japan is because Japan is at Top 10 for having lowest crime rates in the world. Those videos are an another reason for me to visit Japan. Have you ever dreamed of visiting a place you wanted to visit for a long time? A place famous for it's culture, traditions, food, advanced cities, etc. I also want to enjoy those amazing moments like they did.\n",
            "\n",
            "Japan is the country where I would like to visit because of their culture, technology, busiest cities, food and all the other videos which makes me to visit there. I will also try to learn Japanese and understand their tradition and culture. and Japan has also been ranked at Top 5 for being busiest country in the world. Their famous food \"Sushi\" is also an reason for me to visit Japan. This food is liked by many people all over the world.\n",
            "\n",
            "Once I reach there, I will explore all the major cities, eat their famous dishes and watch the anime which is famous over there. Japan is a place where all animes are created\n",
            "Label: AI-generated\n",
            "\n",
            "Classify the following text as 'Human-generated' or 'AI-generated':\n",
            "================================================================================\n",
            "Text: He became the owner of his own brokerage firm and went on to lead a successful life. He was a living testament that one can rise above the circumstances of their life and achieve success against all odds. He rose from a tumultuous childhood and poverty to become a successful entrepreneur, stockbroker, and author. Chris had a great impact on society and his story will continue to be a source of hope and inspiration.. He wrote numerous books and appeared in major films, magnifying his reach and inspiring others.\n",
            "\n",
            "Chris's journey of resilience serves a source of inspiration to many. He put in the work to earn his degree in business administration. He credited his strong will to stay in school and have a better life as a key factor in his success.\n",
            "\n",
            "Thanks to Chris's relentless determination and hard work, he was able to break barriers within the world of banking and finance. \n",
            "Chris Gardner's story is an inspiring one. Despite facing numerous personal and professional challenges, he managed to stay on top of his studies\n",
            "True Label: Human-generated\n",
            "Predicted Label: Human-generated\n",
            "\n",
            "Text: That's what I know about the guide careers, the student want knowlegde from when they're in high chool.. If the school's have guide careers or activities of work the students, they will enjoy that.\n",
            "\n",
            "The seconds reasons is personal but, I see that almost the all people of my country when they come here and they have the opportunity to study but, when they end from the high school they stay in the same thing working in not good jobs earning 14 dollars per hours and some people say now that the diploma from high school is nothing to their. Some people go to school beacuse it's something ilegal, if they not going, I think they will put more interest to the study if they have the opportunity to study what they want to be when they are more olders. The school's need give the opportunity to prepare the students. I know we have young age but we want to be something our life, we have the goals to work in something, we love. I think that the schools need to change to give the student to choose, what they want know, I not talking about the classes, I talking about careers guides. Maybe the all student is not rich's but the only thing they want is work what they want.\n",
            "\n",
            "Everyone enjoy doing the thing what they love with the heart.\n",
            "\n",
            "I lucky to be in this school because the school give the opportunity to some students go to tech. This are examples from others people too.\n",
            "\n",
            "The students dreams that the school have something similar from college.\n",
            "\n",
            "Some people tell me that when I end from high school, I will work in the same jobs, maybe I think the same because my family don't have the money but if the school's have something of academic's to know about the jobs, maybe the academic is not free but, is more cheeper than college or the school's have something that we can know about the jobs. I'm agree to take guides about careers because, I think that the students include to me, we want to have a spacific career guide when, we enter to high school because when we will go to collage, we have to be prepared with ideas, how is the career that we want to take and the students has to be ready with knowlegde with his own careers.\n",
            "\n",
            "The first reason, I say that because the students will be more excite to came to the school, they will say, I like to go to shool beacuse, I practice somenthing that I love\n",
            "True Label: AI-generated\n",
            "Predicted Label: Human-generated\n",
            "\n",
            "Text: Once I reach there, I will eat their famous dishes, explore the major cities, meet many people and watch animes.. Well, Japan is a place which is famous for most of the things I have mentioned.\n",
            "\n",
            "I want to visit Japan because it's a place famous for their culture, tradition, advanced cities, technology and food. Another reason why I want to visit is because, Japan ranks at Top 10 for having low crime rates in the world and Japan ranks Top 5 for being a busiest country in the world.\n",
            "\n",
            "I've seen many YouTubers making video of their stay in Japan and how they enjoyed staying there. Another reason for me to visit Japan is because Japan is at Top 10 for having lowest crime rates in the world. Those videos are an another reason for me to visit Japan. Have you ever dreamed of visiting a place you wanted to visit for a long time? A place famous for it's culture, traditions, food, advanced cities, etc. I also want to enjoy those amazing moments like they did.\n",
            "\n",
            "Japan is the country where I would like to visit because of their culture, technology, busiest cities, food and all the other videos which makes me to visit there. I will also try to learn Japanese and understand their tradition and culture. and Japan has also been ranked at Top 5 for being busiest country in the world. Their famous food \"Sushi\" is also an reason for me to visit Japan. This food is liked by many people all over the world.\n",
            "\n",
            "Once I reach there, I will explore all the major cities, eat their famous dishes and watch the anime which is famous over there. Japan is a place where all animes are created\n",
            "True Label: AI-generated\n",
            "Predicted Label: Human-generated\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def load_test_data(file_path: str) -> Dataset:\n",
        "    \"\"\"Load the test data from a JSONL file into a Hugging Face Dataset.\"\"\"\n",
        "    dataset = load_dataset(\"json\", data_files=file_path, split=\"train\")\n",
        "    return dataset\n",
        "\n",
        "def make_few_shot_prompt(dev_ds: Dataset) -> str:\n",
        "    \"\"\"\n",
        "    Create a few-shot learning prompt for classifying text as 'human' or 'AI' generated.\n",
        "    \n",
        "    Args:\n",
        "        dev_ds (Dataset): Dataset containing text examples and labels.\n",
        "        \n",
        "    Returns:\n",
        "        str: A few-shot prompt string.\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    label_map = {0: \"AI-generated\", 1: \"Human-generated\"}\n",
        "\n",
        "    for example in dev_ds.select(range(3)):  # Take three examples to build the prompt\n",
        "        text = example[\"text\"]\n",
        "        label = example[\"label\"]\n",
        "        label_text = label_map.get(label, \"Human-generated\")  # Default to 'Human-generated'\n",
        "\n",
        "        # Format the example with text and its corresponding label\n",
        "        example_str = f\"Text: {text}\\nLabel: {label_text}\\n\"\n",
        "        examples.append(example_str)\n",
        "\n",
        "    # Combine examples into a few-shot prompt\n",
        "    few_shot_prompt = \"\\n\".join(examples) + \"\\nClassify the following text as 'Human-generated' or 'AI-generated':\"\n",
        "    return few_shot_prompt\n",
        "\n",
        "def classify_text(prompt: str, text: str, model, tokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Classify a single text using the model and the prompt.\n",
        "    \n",
        "    Args:\n",
        "        prompt (str): The few-shot learning prompt.\n",
        "        text (str): The text to classify.\n",
        "        model: The pre-trained language model.\n",
        "        tokenizer: The tokenizer for the language model.\n",
        "        \n",
        "    Returns:\n",
        "        str: The predicted label ('Human-generated' or 'AI-generated').\n",
        "    \"\"\"\n",
        "    input_text = f\"{prompt}\\nText: {text}\\nLabel:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Generate logits for classification\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=5)\n",
        "    \n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract the label from the generated output\n",
        "    if \"AI-generated\" in prediction:\n",
        "        return \"AI-generated\"\n",
        "    elif \"Human-generated\" in prediction:\n",
        "        return \"Human-generated\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "# Paths to model and dataset\n",
        "test_data_path = \"data/test.jsonl\"\n",
        "base_model_path = \"models/pythia-160m\"  # Use your pre-trained language model\n",
        "\n",
        "# Load the test dataset\n",
        "test_dataset = load_test_data(test_data_path)\n",
        "\n",
        "# Limit the test dataset to 3 prompts\n",
        "test_dataset = test_dataset.select(range(3))  # Select only the first 3 prompts\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
        "\n",
        "# Move model to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Generate the few-shot prompt using the first few examples\n",
        "few_shot_prompt = make_few_shot_prompt(test_dataset)\n",
        "\n",
        "# Print the prompt used\n",
        "print(\"Few-Shot Prompt Used:\")\n",
        "print(few_shot_prompt)\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Classify each example in the limited test dataset\n",
        "results = []\n",
        "for example in test_dataset:\n",
        "    text = example[\"text\"]\n",
        "    true_label = example[\"label\"]\n",
        "    predicted_label = classify_text(few_shot_prompt, text, model, tokenizer)\n",
        "    results.append({\"text\": text, \"true_label\": true_label, \"predicted_label\": predicted_label})\n",
        "\n",
        "# Print the results\n",
        "for result in results:\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"True Label: {'Human-generated' if result['true_label'] == 1 else 'AI-generated'}\")\n",
        "    print(f\"Predicted Label: {result['predicted_label']}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
